{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elo_project_Binary classification",
      "provenance": [],
      "mount_file_id": "1dGIu0Z9V1zzSdEWgCkoKR2HVIpmmoadQ",
      "authorship_tag": "ABX9TyM5GS9jEQzR57+Fqa3LKDaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zizilnam/Mini_Project_Machine_Learning_Elo_Merchant_Kaggle/blob/main/Elo_project_Binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyVf48ZZKJwT",
        "outputId": "234ff22e-09ce-48ae-e480-4e881e6b397b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pjlnj4vKykB",
        "outputId": "b10a68ce-a7e6-421c-ff04-21731dc55457"
      },
      "source": [
        "cd/content/drive/MyDrive/KDT/모델링 프로젝트/data/data_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KDT/모델링 프로젝트/data/data_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb9nEViILNHx",
        "outputId": "f4a226cb-5f33-476f-a345-25edf9e6befd"
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHb0uyEbLSZ-"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, roc_auc_score, log_loss, confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "import time\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHQ9VxfBLWEf"
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world\n",
        "#Function to load data into pandas and reduce memory usage\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D05R21q-LZu2",
        "outputId": "738ac2cd-3658-4e29-d4b7-192cc9929640"
      },
      "source": [
        "train = reduce_mem_usage(pd.read_csv(\"/content/drive/MyDrive/KDT/모델링 프로젝트/data/train_features_generated.csv\", index_col=0))\n",
        "test = reduce_mem_usage(pd.read_csv(\"/content/drive/MyDrive/KDT/모델링 프로젝트/data/test_features_generated.csv\", index_col=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 100.71 Mb (74.7% reduction)\n",
            "Mem. usage decreased to 61.19 Mb (74.7% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oOuLjZ8L_NL",
        "outputId": "bf720a7a-fb35-4f1b-9f5b-e1c6487e4d6d"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 201917 entries, 0 to 201916\n",
            "Columns: 257 entries, card_id to rare_datapoints\n",
            "dtypes: float16(194), float32(14), int16(15), int8(33), object(1)\n",
            "memory usage: 100.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF4phfgsMOMF",
        "outputId": "7e01459f-7b0c-4ccf-beab-df0870dba6d2"
      },
      "source": [
        "test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 123623 entries, 0 to 123622\n",
            "Columns: 255 entries, card_id to merchant_active_months_lag12_12.0_std\n",
            "dtypes: float16(195), float32(14), int16(12), int8(33), object(1)\n",
            "memory usage: 61.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qlH5tf9PPs-"
      },
      "source": [
        "train 변수에 결측치가 있는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeBebVnpMPI-",
        "outputId": "1f6c561f-4ead-40ca-f91f-26c812752030"
      },
      "source": [
        "train.isna().sum().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68QztvCPXCF"
      },
      "source": [
        "test 변수에 결측치가 있는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL307sb9PdTX",
        "outputId": "0871bc92-a0aa-4aec-844e-fb5cbf6ff887"
      },
      "source": [
        "test.isna().sum().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMZE3XkkTs50"
      },
      "source": [
        "train에서 아웃라이어를 구분하기 위해 아웃라이어를 표시한 rare_datapoint column을 나눠준다.\n",
        "그리고 train에서 학습에 사용되지 않는 feature들을 제외해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryo2kWmIPgPc"
      },
      "source": [
        "y_data = train['rare_datapoints']\n",
        "train.drop(columns = ['rare_datapoints', 'card_id', 'target'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v3CrHchenBI"
      },
      "source": [
        "test.drop(columns=['card_id'], axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "LxLHAiLujOqK",
        "outputId": "c7bba9ac-39a9-4c8d-a3fa-5c695a706251"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1_1</th>\n",
              "      <th>feature_1_2</th>\n",
              "      <th>feature_1_3</th>\n",
              "      <th>feature_1_4</th>\n",
              "      <th>feature_1_5</th>\n",
              "      <th>feature_2_1</th>\n",
              "      <th>feature_2_2</th>\n",
              "      <th>feature_2_3</th>\n",
              "      <th>feature_3_0</th>\n",
              "      <th>feature_3_1</th>\n",
              "      <th>first_active_month_diff_from_today</th>\n",
              "      <th>year_of_joining</th>\n",
              "      <th>authorized_flag_sum</th>\n",
              "      <th>authorized_flag_mean</th>\n",
              "      <th>category_1_sum</th>\n",
              "      <th>category_1_mean</th>\n",
              "      <th>category_2_1.0_sum</th>\n",
              "      <th>category_2_1.0_mean</th>\n",
              "      <th>category_2_2.0_sum</th>\n",
              "      <th>category_2_2.0_mean</th>\n",
              "      <th>category_2_3.0_sum</th>\n",
              "      <th>category_2_3.0_mean</th>\n",
              "      <th>category_2_4.0_sum</th>\n",
              "      <th>category_2_4.0_mean</th>\n",
              "      <th>category_2_5.0_sum</th>\n",
              "      <th>category_2_5.0_mean</th>\n",
              "      <th>category_3_A_sum</th>\n",
              "      <th>category_3_A_mean</th>\n",
              "      <th>category_3_B_sum</th>\n",
              "      <th>category_3_B_mean</th>\n",
              "      <th>category_3_C_sum</th>\n",
              "      <th>category_3_C_mean</th>\n",
              "      <th>city_id_nunique</th>\n",
              "      <th>state_id_nunique</th>\n",
              "      <th>subsector_id_nunique</th>\n",
              "      <th>merchant_category_id_nunique</th>\n",
              "      <th>merchant_id_nunique</th>\n",
              "      <th>month_lag_sum</th>\n",
              "      <th>month_lag_mean</th>\n",
              "      <th>month_lag_min</th>\n",
              "      <th>...</th>\n",
              "      <th>merchant_active_months_lag6_5.0_std</th>\n",
              "      <th>merchant_active_months_lag6_6.0_sum</th>\n",
              "      <th>merchant_active_months_lag6_6.0_mean</th>\n",
              "      <th>merchant_active_months_lag6_6.0_std</th>\n",
              "      <th>merchant_active_months_lag12_1.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_1.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_1.0_std</th>\n",
              "      <th>merchant_active_months_lag12_2.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_2.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_2.0_std</th>\n",
              "      <th>merchant_active_months_lag12_3.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_3.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_3.0_std</th>\n",
              "      <th>merchant_active_months_lag12_4.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_4.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_4.0_std</th>\n",
              "      <th>merchant_active_months_lag12_5.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_5.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_5.0_std</th>\n",
              "      <th>merchant_active_months_lag12_6.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_6.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_6.0_std</th>\n",
              "      <th>merchant_active_months_lag12_7.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_7.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_7.0_std</th>\n",
              "      <th>merchant_active_months_lag12_8.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_8.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_8.0_std</th>\n",
              "      <th>merchant_active_months_lag12_9.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_9.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_9.0_std</th>\n",
              "      <th>merchant_active_months_lag12_10.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_10.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_10.0_std</th>\n",
              "      <th>merchant_active_months_lag12_11.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_11.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_11.0_std</th>\n",
              "      <th>merchant_active_months_lag12_12.0_sum</th>\n",
              "      <th>merchant_active_months_lag12_12.0_mean</th>\n",
              "      <th>merchant_active_months_lag12_12.0_std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>2017</td>\n",
              "      <td>270</td>\n",
              "      <td>0.954102</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>280.0</td>\n",
              "      <td>0.989258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.010597</td>\n",
              "      <td>279.0</td>\n",
              "      <td>0.98584</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.014137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>46</td>\n",
              "      <td>117</td>\n",
              "      <td>-983.0</td>\n",
              "      <td>-3.472656</td>\n",
              "      <td>-8</td>\n",
              "      <td>...</td>\n",
              "      <td>0.058929</td>\n",
              "      <td>281.0</td>\n",
              "      <td>0.975586</td>\n",
              "      <td>0.154297</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>0.058929</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.017365</td>\n",
              "      <td>0.130859</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.006943</td>\n",
              "      <td>0.083191</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>274.0</td>\n",
              "      <td>0.951172</td>\n",
              "      <td>0.215454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>2017</td>\n",
              "      <td>345</td>\n",
              "      <td>0.969238</td>\n",
              "      <td>31</td>\n",
              "      <td>0.087097</td>\n",
              "      <td>325.0</td>\n",
              "      <td>0.913086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>282.0</td>\n",
              "      <td>0.791992</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.202271</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>58</td>\n",
              "      <td>148</td>\n",
              "      <td>-1752.0</td>\n",
              "      <td>-4.921875</td>\n",
              "      <td>-12</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>371.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.024261</td>\n",
              "      <td>0.154053</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.008087</td>\n",
              "      <td>0.089661</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>359.0</td>\n",
              "      <td>0.967773</td>\n",
              "      <td>0.177124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>2016</td>\n",
              "      <td>42</td>\n",
              "      <td>0.954590</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.090881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.909180</td>\n",
              "      <td>44.0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>-368.0</td>\n",
              "      <td>-8.367188</td>\n",
              "      <td>-13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.022217</td>\n",
              "      <td>0.149048</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.022217</td>\n",
              "      <td>0.149048</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.955566</td>\n",
              "      <td>0.208374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>2017</td>\n",
              "      <td>84</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>0.154785</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.142822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.702148</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.880859</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.083313</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>57</td>\n",
              "      <td>-206.0</td>\n",
              "      <td>-2.453125</td>\n",
              "      <td>-5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.292969</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.174927</td>\n",
              "      <td>2</td>\n",
              "      <td>0.020828</td>\n",
              "      <td>0.143555</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>0.854004</td>\n",
              "      <td>0.354736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>2017</td>\n",
              "      <td>164</td>\n",
              "      <td>0.970215</td>\n",
              "      <td>17</td>\n",
              "      <td>0.100586</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.070984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.041412</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.946777</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.047333</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>37</td>\n",
              "      <td>102</td>\n",
              "      <td>-115.0</td>\n",
              "      <td>-0.680664</td>\n",
              "      <td>-3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074951</td>\n",
              "      <td>176.0</td>\n",
              "      <td>0.988770</td>\n",
              "      <td>0.105713</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.005619</td>\n",
              "      <td>0.074951</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.044952</td>\n",
              "      <td>0.207764</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.016861</td>\n",
              "      <td>0.129028</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>0.926758</td>\n",
              "      <td>0.260986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 254 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_1_1  ...  merchant_active_months_lag12_12.0_std\n",
              "0            0  ...                               0.215454\n",
              "1            0  ...                               0.177124\n",
              "2            0  ...                               0.208374\n",
              "3            0  ...                               0.354736\n",
              "4            1  ...                               0.260986\n",
              "\n",
              "[5 rows x 254 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RimkOFTjXOLC"
      },
      "source": [
        "데이터를 스플릿하고 학습시킨 뒤 평가까지하는 함수를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0ICPLUUOaU"
      },
      "source": [
        "def LGBM_CV(\n",
        "            num_leaves,\n",
        "            max_depth,\n",
        "            n_estimators,\n",
        "            min_split_gain,\n",
        "            subsample,\n",
        "            min_child_samples,\n",
        "            colsample_bytree,\n",
        "            reg_alpha,\n",
        "            reg_lambda\n",
        "):\n",
        "\n",
        "  folds = StratifiedKFold(n_splits = 3, shuffle=True, random_state=5)\n",
        "  oof = np.zeros(train.shape[0])\n",
        "\n",
        "  for fold_, (train_idx, val_idx) in enumerate(folds.split(train, y_data)):\n",
        "    print(\"Fold:\", fold_)\n",
        "    trn_data = lgb.Dataset(train.iloc[train_idx], label = y_data.iloc[train_idx])\n",
        "    val_data = lgb.Dataset(train.iloc[val_idx], label = y_data.iloc[val_idx])\n",
        "\n",
        "    params = {  \n",
        "                'task': 'train',\n",
        "                'objective' : 'binary',\n",
        "                'boosting_type' : 'goss',\n",
        "                'num_leaves' : int(num_leaves),\n",
        "                'max_depth' : int(max_depth),\n",
        "                'learning_rate' : 0.01,\n",
        "                'n_estimators' : int(n_estimators),\n",
        "                'is_unbalance' : True,\n",
        "                'min_split_gain' : min_split_gain,\n",
        "                'subsample' : subsample,\n",
        "                'min_child_samples' : int(min_child_samples),\n",
        "                'colsample_bytree' : colsample_bytree,\n",
        "                'n_jobs' : -1,\n",
        "                'reg_alpha' : reg_alpha,\n",
        "                'reg_lambda' : reg_lambda,\n",
        "                'metric': 'binary_logloss'\n",
        "    }\n",
        "\n",
        "    clf = lgb.train(params,\n",
        "                    trn_data,\n",
        "                    valid_sets = [trn_data, val_data],\n",
        "                    verbose_eval = 1000,\n",
        "                    early_stopping_rounds = 200\n",
        "                    )\n",
        "    \n",
        "    oof[val_idx] = clf.predict(train.iloc[val_idx])\n",
        "  \n",
        "  #oof = np.where(oof > 0.5, 1, 0) # Change predictions to 0-1 output\n",
        "  #Change the error metric if needed\n",
        "  return -log_loss(y_data, oof)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoKl5rIzbjXi"
      },
      "source": [
        "Bayesian Optimization의 하이퍼파라미터 범위를 지정해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiER-PS9W9VN"
      },
      "source": [
        "LGB_BO = BayesianOptimization(LGBM_CV, {\n",
        "    'min_split_gain': (0, 1),\n",
        "    'subsample': (0, 1) ,\n",
        "    'min_child_samples': (10, 200),\n",
        "    'colsample_bytree': (0, 1),\n",
        "    'reg_alpha': (0, 1),\n",
        "    'reg_lambda': (0, 1),\n",
        "    'max_depth': (4, 10),\n",
        "    'num_leaves': (5, 200),\n",
        "    'n_estimators' : (10, 750)\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WvVyhtBkUSO"
      },
      "source": [
        "위의 함수를 이용해 범위 내에 있는 하이퍼 파라미터 중 최적의 값을 선정한다.\n",
        "코드 실행 시간이 길면 얼마나 오래 걸리는 지 궁금하기 때문에 걸린 시간을 보여주는 코드도 쓴다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4vBKHsJYG1B",
        "outputId": "5884fa5a-ce6a-473a-840a-a356ae830869"
      },
      "source": [
        "start_time = time.time()\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)\n",
        "    \n",
        "print(\"Time taken\", time.time()-start_time)\n",
        "print('-'*130)\n",
        "print('Final Results')\n",
        "print('Maximum  value: %f' % LGB_BO.max['target'])\n",
        "print('Best  parameters: ', LGB_BO.max['params'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | colsam... | max_depth | min_ch... | min_sp... | n_esti... | num_le... | reg_alpha | reg_la... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0585984\tvalid_1's binary_logloss: 0.0589774\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0586433\tvalid_1's binary_logloss: 0.0589461\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0587114\tvalid_1's binary_logloss: 0.0589054\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.05894 \u001b[0m | \u001b[0m 0.11    \u001b[0m | \u001b[0m 5.842   \u001b[0m | \u001b[0m 22.89   \u001b[0m | \u001b[0m 0.568   \u001b[0m | \u001b[0m 744.7   \u001b[0m | \u001b[0m 33.42   \u001b[0m | \u001b[0m 0.6092  \u001b[0m | \u001b[0m 0.4921  \u001b[0m | \u001b[0m 0.6763  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.05771\tvalid_1's binary_logloss: 0.0590878\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0578036\tvalid_1's binary_logloss: 0.0591958\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0578308\tvalid_1's binary_logloss: 0.0589975\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.05909 \u001b[0m | \u001b[0m 0.1094  \u001b[0m | \u001b[0m 7.514   \u001b[0m | \u001b[0m 174.0   \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 199.1   \u001b[0m | \u001b[0m 174.7   \u001b[0m | \u001b[0m 0.7184  \u001b[0m | \u001b[0m 0.328   \u001b[0m | \u001b[0m 0.8475  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0566386\tvalid_1's binary_logloss: 0.0568946\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0566199\tvalid_1's binary_logloss: 0.0567412\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0566086\tvalid_1's binary_logloss: 0.0567983\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.05681 \u001b[0m | \u001b[95m 0.8215  \u001b[0m | \u001b[95m 6.185   \u001b[0m | \u001b[95m 14.85   \u001b[0m | \u001b[95m 0.3927  \u001b[0m | \u001b[95m 25.04   \u001b[0m | \u001b[95m 9.001   \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 0.9166  \u001b[0m | \u001b[95m 0.2484  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567893\tvalid_1's binary_logloss: 0.0570493\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567743\tvalid_1's binary_logloss: 0.056924\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568441\tvalid_1's binary_logloss: 0.0570449\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.05701 \u001b[0m | \u001b[0m 0.3094  \u001b[0m | \u001b[0m 7.606   \u001b[0m | \u001b[0m 198.2   \u001b[0m | \u001b[0m 0.2393  \u001b[0m | \u001b[0m 27.0    \u001b[0m | \u001b[0m 10.8    \u001b[0m | \u001b[0m 0.8646  \u001b[0m | \u001b[0m 0.7961  \u001b[0m | \u001b[0m 0.6953  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569573\tvalid_1's binary_logloss: 0.0572086\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568977\tvalid_1's binary_logloss: 0.0571036\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.056911\tvalid_1's binary_logloss: 0.0571611\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.05716 \u001b[0m | \u001b[0m 0.1819  \u001b[0m | \u001b[0m 4.473   \u001b[0m | \u001b[0m 12.42   \u001b[0m | \u001b[0m 0.2705  \u001b[0m | \u001b[0m 19.29   \u001b[0m | \u001b[0m 13.03   \u001b[0m | \u001b[0m 0.4182  \u001b[0m | \u001b[0m 0.9623  \u001b[0m | \u001b[0m 0.253   \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.057078\tvalid_1's binary_logloss: 0.0572816\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570653\tvalid_1's binary_logloss: 0.0571912\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571156\tvalid_1's binary_logloss: 0.0573326\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.05727 \u001b[0m | \u001b[0m 0.346   \u001b[0m | \u001b[0m 4.763   \u001b[0m | \u001b[0m 13.51   \u001b[0m | \u001b[0m 0.515   \u001b[0m | \u001b[0m 12.9    \u001b[0m | \u001b[0m 7.582   \u001b[0m | \u001b[0m 0.7118  \u001b[0m | \u001b[0m 0.1995  \u001b[0m | \u001b[0m 0.626   \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571522\tvalid_1's binary_logloss: 0.0572762\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571342\tvalid_1's binary_logloss: 0.0571282\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570789\tvalid_1's binary_logloss: 0.0572204\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.05721 \u001b[0m | \u001b[0m 0.8946  \u001b[0m | \u001b[0m 6.577   \u001b[0m | \u001b[0m 181.7   \u001b[0m | \u001b[0m 0.3272  \u001b[0m | \u001b[0m 34.71   \u001b[0m | \u001b[0m 5.289   \u001b[0m | \u001b[0m 0.4643  \u001b[0m | \u001b[0m 0.3277  \u001b[0m | \u001b[0m 0.6419  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567941\tvalid_1's binary_logloss: 0.0569876\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568144\tvalid_1's binary_logloss: 0.0568691\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567608\tvalid_1's binary_logloss: 0.0569007\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.05692 \u001b[0m | \u001b[0m 0.9215  \u001b[0m | \u001b[0m 7.291   \u001b[0m | \u001b[0m 189.3   \u001b[0m | \u001b[0m 0.1466  \u001b[0m | \u001b[0m 21.47   \u001b[0m | \u001b[0m 7.557   \u001b[0m | \u001b[0m 0.6453  \u001b[0m | \u001b[0m 0.1071  \u001b[0m | \u001b[0m 0.3441  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0563897\tvalid_1's binary_logloss: 0.0566948\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0564492\tvalid_1's binary_logloss: 0.0566704\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.05644\tvalid_1's binary_logloss: 0.0567621\n",
            "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.05671 \u001b[0m | \u001b[95m 0.6519  \u001b[0m | \u001b[95m 7.179   \u001b[0m | \u001b[95m 197.4   \u001b[0m | \u001b[95m 0.4381  \u001b[0m | \u001b[95m 22.3    \u001b[0m | \u001b[95m 12.49   \u001b[0m | \u001b[95m 0.03128 \u001b[0m | \u001b[95m 0.1584  \u001b[0m | \u001b[95m 0.6558  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.05693\tvalid_1's binary_logloss: 0.0571005\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569159\tvalid_1's binary_logloss: 0.0570683\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.056954\tvalid_1's binary_logloss: 0.057082\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.05708 \u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 9.346   \u001b[0m | \u001b[0m 17.15   \u001b[0m | \u001b[0m 0.8075  \u001b[0m | \u001b[0m 25.99   \u001b[0m | \u001b[0m 8.202   \u001b[0m | \u001b[0m 0.2076  \u001b[0m | \u001b[0m 0.839   \u001b[0m | \u001b[0m 0.02604 \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569927\tvalid_1's binary_logloss: 0.0572031\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569817\tvalid_1's binary_logloss: 0.0571303\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569907\tvalid_1's binary_logloss: 0.0572173\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.05718 \u001b[0m | \u001b[0m 0.332   \u001b[0m | \u001b[0m 4.699   \u001b[0m | \u001b[0m 191.2   \u001b[0m | \u001b[0m 0.9773  \u001b[0m | \u001b[0m 18.6    \u001b[0m | \u001b[0m 8.427   \u001b[0m | \u001b[0m 0.2989  \u001b[0m | \u001b[0m 0.6698  \u001b[0m | \u001b[0m 0.8078  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568602\tvalid_1's binary_logloss: 0.057062\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567926\tvalid_1's binary_logloss: 0.0569624\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568883\tvalid_1's binary_logloss: 0.0570483\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.05702 \u001b[0m | \u001b[0m 0.5516  \u001b[0m | \u001b[0m 7.761   \u001b[0m | \u001b[0m 11.13   \u001b[0m | \u001b[0m 0.2013  \u001b[0m | \u001b[0m 32.5    \u001b[0m | \u001b[0m 9.81    \u001b[0m | \u001b[0m 0.3992  \u001b[0m | \u001b[0m 0.1627  \u001b[0m | \u001b[0m 0.8417  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571522\tvalid_1's binary_logloss: 0.0572762\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571342\tvalid_1's binary_logloss: 0.0571282\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570789\tvalid_1's binary_logloss: 0.0572204\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.05721 \u001b[0m | \u001b[0m 0.8134  \u001b[0m | \u001b[0m 7.75    \u001b[0m | \u001b[0m 198.9   \u001b[0m | \u001b[0m 0.4136  \u001b[0m | \u001b[0m 32.53   \u001b[0m | \u001b[0m 5.792   \u001b[0m | \u001b[0m 0.4017  \u001b[0m | \u001b[0m 0.3354  \u001b[0m | \u001b[0m 0.02716 \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0565361\tvalid_1's binary_logloss: 0.0568297\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0565165\tvalid_1's binary_logloss: 0.0567901\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0566069\tvalid_1's binary_logloss: 0.0568883\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.05684 \u001b[0m | \u001b[0m 0.5015  \u001b[0m | \u001b[0m 9.234   \u001b[0m | \u001b[0m 15.82   \u001b[0m | \u001b[0m 0.1412  \u001b[0m | \u001b[0m 19.66   \u001b[0m | \u001b[0m 13.93   \u001b[0m | \u001b[0m 0.8748  \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 0.2006  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570568\tvalid_1's binary_logloss: 0.057238\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571077\tvalid_1's binary_logloss: 0.057243\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569958\tvalid_1's binary_logloss: 0.0571971\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.05723 \u001b[0m | \u001b[0m 0.1827  \u001b[0m | \u001b[0m 8.079   \u001b[0m | \u001b[0m 19.6    \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 11.16   \u001b[0m | \u001b[0m 10.1    \u001b[0m | \u001b[0m 0.6827  \u001b[0m | \u001b[0m 0.08982 \u001b[0m | \u001b[0m 0.2434  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571138\tvalid_1's binary_logloss: 0.0572487\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571091\tvalid_1's binary_logloss: 0.0572113\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571452\tvalid_1's binary_logloss: 0.0572294\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.05723 \u001b[0m | \u001b[0m 0.5508  \u001b[0m | \u001b[0m 4.282   \u001b[0m | \u001b[0m 195.3   \u001b[0m | \u001b[0m 0.5093  \u001b[0m | \u001b[0m 16.71   \u001b[0m | \u001b[0m 6.024   \u001b[0m | \u001b[0m 0.8324  \u001b[0m | \u001b[0m 0.5825  \u001b[0m | \u001b[0m 0.7931  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570039\tvalid_1's binary_logloss: 0.0571799\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0569994\tvalid_1's binary_logloss: 0.057107\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570493\tvalid_1's binary_logloss: 0.0571381\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.05714 \u001b[0m | \u001b[0m 0.5109  \u001b[0m | \u001b[0m 8.673   \u001b[0m | \u001b[0m 11.45   \u001b[0m | \u001b[0m 0.09534 \u001b[0m | \u001b[0m 13.31   \u001b[0m | \u001b[0m 7.109   \u001b[0m | \u001b[0m 0.2658  \u001b[0m | \u001b[0m 0.1426  \u001b[0m | \u001b[0m 0.2343  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567819\tvalid_1's binary_logloss: 0.0569957\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0568885\tvalid_1's binary_logloss: 0.0569563\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0567685\tvalid_1's binary_logloss: 0.0569031\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.05695 \u001b[0m | \u001b[0m 0.5871  \u001b[0m | \u001b[0m 7.407   \u001b[0m | \u001b[0m 197.2   \u001b[0m | \u001b[0m 0.4472  \u001b[0m | \u001b[0m 10.61   \u001b[0m | \u001b[0m 7.688   \u001b[0m | \u001b[0m 0.4791  \u001b[0m | \u001b[0m 0.3365  \u001b[0m | \u001b[0m 0.9493  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571517\tvalid_1's binary_logloss: 0.0572757\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0571335\tvalid_1's binary_logloss: 0.0571276\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0570781\tvalid_1's binary_logloss: 0.0572197\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.05721 \u001b[0m | \u001b[0m 0.6832  \u001b[0m | \u001b[0m 9.845   \u001b[0m | \u001b[0m 12.07   \u001b[0m | \u001b[0m 0.9544  \u001b[0m | \u001b[0m 25.49   \u001b[0m | \u001b[0m 5.876   \u001b[0m | \u001b[0m 0.4976  \u001b[0m | \u001b[0m 0.5191  \u001b[0m | \u001b[0m 0.04822 \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0519833\tvalid_1's binary_logloss: 0.0551583\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0515663\tvalid_1's binary_logloss: 0.054733\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0520272\tvalid_1's binary_logloss: 0.0550207\n",
            "| \u001b[95m 20      \u001b[0m | \u001b[95m-0.05497 \u001b[0m | \u001b[95m 0.9167  \u001b[0m | \u001b[95m 9.633   \u001b[0m | \u001b[95m 21.23   \u001b[0m | \u001b[95m 0.6328  \u001b[0m | \u001b[95m 13.74   \u001b[0m | \u001b[95m 194.8   \u001b[0m | \u001b[95m 0.7155  \u001b[0m | \u001b[95m 0.3936  \u001b[0m | \u001b[95m 0.04018 \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0520613\tvalid_1's binary_logloss: 0.0550825\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0513779\tvalid_1's binary_logloss: 0.054792\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0524009\tvalid_1's binary_logloss: 0.0552687\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.05505 \u001b[0m | \u001b[0m 0.7614  \u001b[0m | \u001b[0m 9.164   \u001b[0m | \u001b[0m 14.73   \u001b[0m | \u001b[0m 0.4699  \u001b[0m | \u001b[0m 16.71   \u001b[0m | \u001b[0m 189.4   \u001b[0m | \u001b[0m 0.7236  \u001b[0m | \u001b[0m 0.4376  \u001b[0m | \u001b[0m 0.3014  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0546592\tvalid_1's binary_logloss: 0.056167\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0542931\tvalid_1's binary_logloss: 0.0562054\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0547566\tvalid_1's binary_logloss: 0.0558879\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.05609 \u001b[0m | \u001b[0m 0.6017  \u001b[0m | \u001b[0m 7.136   \u001b[0m | \u001b[0m 26.52   \u001b[0m | \u001b[0m 0.5765  \u001b[0m | \u001b[0m 16.33   \u001b[0m | \u001b[0m 199.7   \u001b[0m | \u001b[0m 0.2197  \u001b[0m | \u001b[0m 0.4992  \u001b[0m | \u001b[0m 0.3121  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0558596\tvalid_1's binary_logloss: 0.0563331\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0557167\tvalid_1's binary_logloss: 0.0562426\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0558013\tvalid_1's binary_logloss: 0.0562617\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.05628 \u001b[0m | \u001b[0m 0.8871  \u001b[0m | \u001b[0m 5.363   \u001b[0m | \u001b[0m 16.42   \u001b[0m | \u001b[0m 0.9044  \u001b[0m | \u001b[0m 46.26   \u001b[0m | \u001b[0m 198.4   \u001b[0m | \u001b[0m 0.4426  \u001b[0m | \u001b[0m 0.344   \u001b[0m | \u001b[0m 0.1517  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0573849\tvalid_1's binary_logloss: 0.0583586\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0574816\tvalid_1's binary_logloss: 0.0583466\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0576085\tvalid_1's binary_logloss: 0.0582464\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.05832 \u001b[0m | \u001b[0m 0.158   \u001b[0m | \u001b[0m 6.02    \u001b[0m | \u001b[0m 10.33   \u001b[0m | \u001b[0m 0.1118  \u001b[0m | \u001b[0m 27.8    \u001b[0m | \u001b[0m 199.0   \u001b[0m | \u001b[0m 0.3357  \u001b[0m | \u001b[0m 0.9627  \u001b[0m | \u001b[0m 0.2792  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0557452\tvalid_1's binary_logloss: 0.0562233\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0557524\tvalid_1's binary_logloss: 0.0562131\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0558143\tvalid_1's binary_logloss: 0.0562843\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.05624 \u001b[0m | \u001b[0m 0.978   \u001b[0m | \u001b[0m 5.244   \u001b[0m | \u001b[0m 199.3   \u001b[0m | \u001b[0m 0.6978  \u001b[0m | \u001b[0m 37.97   \u001b[0m | \u001b[0m 198.6   \u001b[0m | \u001b[0m 0.6852  \u001b[0m | \u001b[0m 0.1873  \u001b[0m | \u001b[0m 0.3685  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0549424\tvalid_1's binary_logloss: 0.0565616\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.054538\tvalid_1's binary_logloss: 0.05623\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0548205\tvalid_1's binary_logloss: 0.0563128\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.05637 \u001b[0m | \u001b[0m 0.2175  \u001b[0m | \u001b[0m 8.003   \u001b[0m | \u001b[0m 181.4   \u001b[0m | \u001b[0m 0.9701  \u001b[0m | \u001b[0m 36.58   \u001b[0m | \u001b[0m 195.3   \u001b[0m | \u001b[0m 0.8679  \u001b[0m | \u001b[0m 0.947   \u001b[0m | \u001b[0m 0.711   \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0584891\tvalid_1's binary_logloss: 0.0588841\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0584375\tvalid_1's binary_logloss: 0.0588166\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0585097\tvalid_1's binary_logloss: 0.05879\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.05883 \u001b[0m | \u001b[0m 0.1194  \u001b[0m | \u001b[0m 5.007   \u001b[0m | \u001b[0m 11.08   \u001b[0m | \u001b[0m 0.6708  \u001b[0m | \u001b[0m 21.12   \u001b[0m | \u001b[0m 196.7   \u001b[0m | \u001b[0m 0.1286  \u001b[0m | \u001b[0m 0.1737  \u001b[0m | \u001b[0m 0.02684 \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0531447\tvalid_1's binary_logloss: 0.0552431\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0531289\tvalid_1's binary_logloss: 0.0553144\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[3]\ttraining's binary_logloss: 0.0530548\tvalid_1's binary_logloss: 0.0552897\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.05528 \u001b[0m | \u001b[0m 0.8351  \u001b[0m | \u001b[0m 9.226   \u001b[0m | \u001b[0m 191.0   \u001b[0m | \u001b[0m 0.1523  \u001b[0m | \u001b[0m 744.0   \u001b[0m | \u001b[0m 193.4   \u001b[0m | \u001b[0m 0.5107  \u001b[0m | \u001b[0m 0.7415  \u001b[0m | \u001b[0m 0.2687  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.05591\tvalid_1's binary_logloss: 0.0563751\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0557999\tvalid_1's binary_logloss: 0.0562964\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0559864\tvalid_1's binary_logloss: 0.0565048\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.05639 \u001b[0m | \u001b[0m 0.8366  \u001b[0m | \u001b[0m 5.627   \u001b[0m | \u001b[0m 195.6   \u001b[0m | \u001b[0m 0.7908  \u001b[0m | \u001b[0m 736.8   \u001b[0m | \u001b[0m 197.1   \u001b[0m | \u001b[0m 0.3256  \u001b[0m | \u001b[0m 0.1841  \u001b[0m | \u001b[0m 0.762   \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0539084\tvalid_1's binary_logloss: 0.0559074\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0526423\tvalid_1's binary_logloss: 0.0557302\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[2]\ttraining's binary_logloss: 0.0539816\tvalid_1's binary_logloss: 0.055957\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.05586 \u001b[0m | \u001b[0m 0.4937  \u001b[0m | \u001b[0m 9.996   \u001b[0m | \u001b[0m 184.3   \u001b[0m | \u001b[0m 0.4021  \u001b[0m | \u001b[0m 729.1   \u001b[0m | \u001b[0m 197.6   \u001b[0m | \u001b[0m 0.7687  \u001b[0m | \u001b[0m 0.0468  \u001b[0m | \u001b[0m 0.7728  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0574329\tvalid_1's binary_logloss: 0.0586522\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.0573877\tvalid_1's binary_logloss: 0.0586431\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[1]\ttraining's binary_logloss: 0.057586\tvalid_1's binary_logloss: 0.0586031\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.05863 \u001b[0m | \u001b[0m 0.08332 \u001b[0m | \u001b[0m 9.528   \u001b[0m | \u001b[0m 192.9   \u001b[0m | \u001b[0m 0.726   \u001b[0m | \u001b[0m 747.5   \u001b[0m | \u001b[0m 197.3   \u001b[0m | \u001b[0m 0.2234  \u001b[0m | \u001b[0m 0.716   \u001b[0m | \u001b[0m 0.9005  \u001b[0m |\n",
            "Fold: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0531592\tvalid_1's binary_logloss: 0.0555087\n",
            "Fold: 1\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.052959\tvalid_1's binary_logloss: 0.0553575\n",
            "Fold: 2\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[4]\ttraining's binary_logloss: 0.0534136\tvalid_1's binary_logloss: 0.05559\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.05549 \u001b[0m | \u001b[0m 0.7717  \u001b[0m | \u001b[0m 9.669   \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 0.05605 \u001b[0m | \u001b[0m 13.57   \u001b[0m | \u001b[0m 188.1   \u001b[0m | \u001b[0m 0.4664  \u001b[0m | \u001b[0m 0.9795  \u001b[0m | \u001b[0m 0.6266  \u001b[0m |\n",
            "=====================================================================================================================================\n",
            "Time taken 1094.5479657649994\n",
            "----------------------------------------------------------------------------------------------------------------------------------\n",
            "Final Results\n",
            "Maximum  value: -0.054971\n",
            "Best  parameters:  {'colsample_bytree': 0.916696198086027, 'max_depth': 9.633270599649617, 'min_child_samples': 21.22845335671823, 'min_split_gain': 0.6328282464527148, 'n_estimators': 13.73601660809571, 'num_leaves': 194.78993625112014, 'reg_alpha': 0.7154684277016825, 'reg_lambda': 0.3936289516480598, 'subsample': 0.040175028233326415}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkSxOIRfrbfk"
      },
      "source": [
        "'colsample_bytree': 0.916696198086027, 'max_depth': 9.633270599649617, 'min_child_samples': 21.22845335671823, 'min_split_gain': 0.6328282464527148, 'n_estimators': 13.73601660809571, 'num_leaves': 194.78993625112014, 'reg_alpha': 0.7154684277016825, 'reg_lambda': 0.3936289516480598, 'subsample': 0.040175028233326415"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLtRMANyfiGQ"
      },
      "source": [
        "# Training with the best parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y87CzK2RcgAu"
      },
      "source": [
        "위 코드에서 구한 최고의 하이퍼 파라미터로 학습시켜봅니다.\n",
        "param dict에 베스트 파라미터를 입력합니다.\n",
        "leaves, max_depth, min_child_samples는 int의 형태를 갖고 있어야해서 반올림해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3--ZsSnVpHN_"
      },
      "source": [
        "params = {\n",
        "          'colsample_bytree': 0.916696198086027,\n",
        "          'max_depth': 10,\n",
        "          'min_child_samples': 21,\n",
        "          'min_split_gain': 0.6328282464527148,\n",
        "          'n_estimators': 13.73601660809571,\n",
        "          'num_leaves': 195,\n",
        "          'reg_alpha': 0.7154684277016825,\n",
        "          'reg_lambda': 0.3936289516480598,\n",
        "          'subsample': 0.040175028233326415\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEXUXCIypi18"
      },
      "source": [
        "trn_data = lgb.Dataset(train, label=y_data)\n",
        "\n",
        "clf = lgb.train(params,\n",
        "                trn_data,\n",
        "                10000,\n",
        "                verbose_eval=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2OLt80xp3ao"
      },
      "source": [
        "train_preds = clf.predict(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hvgxaLDqqlc"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVmx8APKqt4r"
      },
      "source": [
        "with open('Binary_Classification.pkl', 'wb') as f:\n",
        "  pickle.dump(clf, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DidZsowgq6sT"
      },
      "source": [
        "with open('Binary_Classification.pkl', 'rb') as f:\n",
        "  clf = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BgZ_dGdrG3U"
      },
      "source": [
        "# Result:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zqRrJUjrgKm"
      },
      "source": [
        "AUC score : out lier냐 아니냐를 구분하는 모델의 정확도를 구해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ilR0zkZFrok7",
        "outputId": "d68423cf-f89b-4665-87ec-b691f47f9e74"
      },
      "source": [
        "plt.plot([0,1], [0,1], 'k--')\n",
        "\n",
        "fpr, tpr, threshold = roc_curve(y_data, train_preds)\n",
        "auc_score = roc_auc_score(y_data, train_preds)\n",
        "plt.plot(fpr, tpr, label=\"train AUC:\" +str(auc_score))\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT56O-CYsJMz"
      },
      "source": [
        "Confusion Matrix도 만들어 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS8Z2F2PsgWl"
      },
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "        plt.grid(False)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "2TEz6sh8vKD3",
        "outputId": "cc770b49-c6f3-43d8-9084-09c62b4ed325"
      },
      "source": [
        "\n",
        "#This is decided based on trial and error\n",
        "threshold = 0.030\n",
        "train_pred_labels = np.where(train_preds > threshold, 1, 0)\n",
        "#test_predictions_labels = np.where(test_predictions > threshold, 1, 0)\n",
        "\n",
        "print(\"Train accuracy:\", accuracy_score(train_pred_labels, y_data))\n",
        "print('*'*50)\n",
        "plot_confusion_matrix(confusion_matrix(train_pred_labels, y_data), target_names=[0,1], normalize=False, cmap= plt.get_cmap('plasma_r'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.9403913489205961\n",
            "**************************************************\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAF3CAYAAAAiitViAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxddX3/8dcnExISICQQhGxAxIACpREoO8hSICAaVGSrgBSNClgrLmDrT6iK1latUjEYJBKksggiiLEQcUFRLEGQTYSwSYYsZCEJBEKWz++Pe2bmTJgtMxkm5+b19HEeufdzlvu9l3He8/2e7zk3MhNJkupBv75ugCRJ64uhJkmqG4aaJKluGGqSpLphqEmS6oahJkmqG4aaXlcRMSgifhIRSyLihz04zj9ExO3rs219JSIOjoi/9HU7pHoQXqemtkTEqcB5wJuBZcD9wMWZ+dseHvc04KPAAZm5qscN3cBFRALjMnNWX7dF2hjYU9NrRMR5wDeALwHbAtsD3wYmrofD7wA8tjEEWldERP++boNUTww1tRIRWwKfB87JzB9l5kuZuTIzf5KZnyq2GRgR34iI54rlGxExsFh3aETMjohPRMT8iJgTEWcW6/4N+BxwUkS8GBFnRcRFEXF16fV3jIhs+mUfEe+PiCcjYllEPBUR/1Cq/7a03wERcU8xrHlPRBxQWveriPhCRNxVHOf2iBjezvtvav+nS+0/PiKOjYjHImJRRPxLaft9IuL3EfFCse23ImJAse7OYrM/Fe/3pNLxz4+IucD3mmrFPjsVr7Fn8XxkRDwfEYf26D+stJEw1LS2/YFNgZs62OZfgf2A8cDfAvsAny2t3w7YEhgFnAVcGhHDMvNCar2/6zJz88y8oqOGRMRmwCXAMZm5BXAAtWHQtbfbCvhpse3WwNeBn0bE1qXNTgXOBN4ADAA+2cFLb0ftMxhFLYQvB94H7AUcDPy/iBhbbLsa+DgwnNpndwRwNkBmHlJs87fF+72udPytqPVaJ5VfODOfAM4Hro6IwcD3gGmZ+asO2iupYKhpbVsDCzoZHvwH4POZOT8znwf+DTittH5lsX5lZk4HXgR26WZ71gC7R8SgzJyTmQ+3sc3bgccz8/uZuSozrwEeBd5R2uZ7mflYZr4MXE8tkNuzktr5w5XAtdQC65uZuax4/UeohTmZeW9m3l287tPAd4C3deE9XZiZK4r2tJKZlwOzgD8AI6j9ESGpCww1rW0hMLyTcz0jgWdKz58pas3HWCsUlwObr2tDMvMl4CTgw8CciPhpRLy5C+1patOo0vO569CehZm5unjcFDrzSutfbto/InaOiFsjYm5ELKXWE21zaLPk+cx8pZNtLgd2B/47M1d0sq2kgqGmtf0eWAEc38E2z1EbOmuyfVHrjpeAwaXn25VXZuZtmXkktR7Lo9R+2XfWnqY2NXazTetiMrV2jcvMIcC/ANHJPh1OOY6IzalN1LkCuKgYXpXUBYaaWsnMJdTOI11aTJAYHBGbRMQxEfEfxWbXAJ+NiG2KCRefA65u75iduB84JCK2LyapfKZpRURsGxETi3NrK6gNY65p4xjTgZ0j4tSI6B8RJwG7Ard2s03rYgtgKfBi0Yv8yFrr5wFvXMdjfhOYmZkfoHau8LIet1LaSBhqeo3M/Bq1a9Q+CzwPPAucC/y42OSLwEzgAeBB4I9FrTuvNQO4rjjWvbQOon5FO54DFlE7V7V2aJCZC4HjgE9QGz79NHBcZi7oTpvW0SepTUJZRq0Xed1a6y8CphWzI0/s7GARMRGYQMv7PA/Ys2nWp6SOefG1JKlu2FOTJNUNQ02SVDcMNUlS3TDUJEl1w1CTJNWNXrlDeMRm2S+G9cahpXWyx1uf7+smSAD89ZlVLFywurML89dZ/4adM3N5t/dfk423ZeaE9dikPtUrodYvhjF4wDm9cWhpnfzyd23dgER6/R12QO/c4CZzOZsNPLfb+y975TOd3datUvwuJ0mqsAAacr13ACvLUJOkimvo9HajGw9DTZIqLDDUypz9KEmqG/bUJKnCgqAh7Z80MdQkqeL6OfzYzFCTpIoz1FoYapJUYU4Uac2BWElS3bCnJkmVFg4/lhhqklRhgefUygw1Saq4ftnXLdhweE5NklQ37KlJUoU5/NiaoSZJFWeotTDUJKniDLUWhpokVVg4pb8VJ4pIkuqGoSZJFRc9+F+nx46YGhHzI+KhUu26iLi/WJ6OiPuL+o4R8XJp3WWlffaKiAcjYlZEXBIRUdS3iogZEfF48e+woh7FdrMi4oGI2LMrn4WhJkkV168YguzO0gVXAhPKhcw8KTPHZ+Z44EbgR6XVTzSty8wPl+qTgQ8C44ql6ZgXAHdk5jjgjuI5wDGlbScV+3fhs5AkVVZtSn/3l85k5p3AojZfu9bbOhG4psM2RowAhmTm3ZmZwFXA8cXqicC04vG0tepXZc3dwNDiOB0y1CSpyhL6ZXR76aGDgXmZ+XipNjYi7ouIX0fEwUVtFDC7tM3sogawbWbOKR7PBbYt7fNsO/u0y9mPkrRxGx4RM0vPp2TmlC7uewqte2lzgO0zc2FE7AX8OCJ262pDMjMjokc3/TLUJKnC1sMdRRZk5t7r/LoR/YF3A3s11TJzBbCieHxvRDwB7Aw0AqNLu48uagDzImJEZs4phhfnF/VGYEw7+7TL4UdJqrSezH3sURj+PfBoZjYPK0bENhHRUDx+I7VJHk8Ww4tLI2K/4jzc6cDNxW63AGcUj89Yq356MQtyP2BJaZiyXfbUJKnierN3EhHXAIdSG6acDVyYmVcAJ/PaCSKHAJ+PiJXAGuDDmdk0yeRsajMpBwE/KxaAfweuj4izgGeoTTwBmA4cC8wClgNndqW9hpokqV2ZeUo79fe3UbuR2hT/trafCezeRn0hcEQb9QTOWcfmGmqSVGXepb81Q02SKs5Qa2GoSVLFGWktDDVJqjCHH1tzSr8kqW7YU5OkilsPt7uqG4aaJFWckdbCUJOkCvObr1vznJokqW7YU5OkirN30sJQk6SK6+GNieuKoSZJFeZ1aq0ZapJUcT37Ws364lCsJKlu2FOTpIpz+LGFoSZJFVY7p6YmhpokVVo4+7HEUJOkirOn1sLPQpJUN+ypSVKFeZ1aa4aaJFWc16m1MNQkqeI8j9TCz0KSVDfsqUlShQXe0LjMUJOkinPIrYWhJkkVZ0+thaEmSRXmbbJa87OQJNUNe2qSVHH2TloYapJUcZGeU2tiqElShXlOrTVDTZIqzn5aCwNeklQ37KlJUsXZO2nhZyFJlRY9+l+nR4+YGhHzI+KhUu2iiGiMiPuL5djSus9ExKyI+EtEHF2qTyhqsyLiglJ9bET8oahfFxEDivrA4vmsYv2OXfk0DDVJqrCmiSLdXbrgSmBCG/X/yszxxTIdICJ2BU4Gdiv2+XZENEREA3ApcAywK3BKsS3AV4pjvQlYDJxV1M8CFhf1/yq265ShJklqV2beCSzq4uYTgWszc0VmPgXMAvYpllmZ+WRmvgpcC0yMiAAOB24o9p8GHF861rTi8Q3AEcX2HTLUJKniogdLD5wbEQ8Uw5PDitoo4NnSNrOLWnv1rYEXMnPVWvVWxyrWLym275ChJkkV18Phx+ERMbO0TOrCS04GdgLGA3OAr63Ht9Mjzn6UpApbDxdfL8jMvddlh8yc1/z6EZcDtxZPG4ExpU1HFzXaqS8EhkZE/6I3Vt6+6VizI6I/sGWxfYfsqUlSxb3ew48RMaL09F1A08zIW4CTi5mLY4FxwP8B9wDjipmOA6hNJrklMxP4JXBCsf8ZwM2lY51RPD4B+EWxfYfsqUmS2hUR1wCHUhumnA1cCBwaEeOBBJ4GPgSQmQ9HxPXAI8Aq4JzMXF0c51zgNqABmJqZDxcvcT5wbUR8EbgPuKKoXwF8PyJmUZuocnJX2muoSVLF9eaQW2ae0kb5ijZqTdtfDFzcRn06ML2N+pPUZkeuXX8FeO86NRZDTZIqLYDOJ7p3oNMBvWox1CSp4pwc0cJQW4++/Z1jmXDMm3j++eXsu9d3AfibPd7AN/97AgM37c+qVWs472O3ce/MOQwZMpDvfu8djB4zhP79+3HJN/7A1Vc9+JpjbrnlQL41+Vh23W0bMpOzPzSd//tDI1d+fyLjdq5dsrHl0IEseWEFB+47FYBPfGp/Tnv/37Jm9Ro+dd4M7vj5U6/fh6DKuexbS5g2dRkknP6PW/CRj27Jgw+s4BMfXciLL65h+x36M+XKNzBkSO1X50MPvsp55y5g2dI1RD/4xV0j2XRTf632Je/S38JQW4/+5/sP8p3J9zLlinc0177wpcP58sW/ZcbtT3LU0TvxhS8dxrFH/YBJH96TR/+8gBPfcwPDhw/i3gc+xHXXPMzKlWtaHfM/vnYkP5/xJKedehObbNKPwYM3AeD9p93cvM2X/v1wlixdAcAub96a97z3Lezz1ssZMXJzbpl+Cm/d/TusWVNnYwxaLx55+FWmTV3GHb8dyYABwQnvmMvRxw7mYx9ZwBe+vBUHHjKIq69cxn9/fQn/etEwVq1KPnTmfC6bug1/s8dAFi1czSab+CtVGw7/vFqP7vrtsyxe/EqrWmayxZCBAAzZciBz5rxY1GHzLWr1zTYfwOLFr7BqVetAGzJkIAccNIZp3/sTACtXrmHJkhWved13nfAWbrjuEQCOe8fO3PjDP/Pqq6t55uklPPnEYvb+u5Hr942qbjz26Er2/ruBDB7cj/79gwMP3pSf/PglZj2+kgMO3hSAQ48YxE9+/BIAv/j5y+y2+wD+Zo/az+5WWzfQ0GCo9aXX4d6PlVKP72mDcsEnf84Xv3wYf551Dhd/+XAu+n+/AuA7k+9llzdvzeNPfZS7Z36A8z8xg7WvwNhhxy1Z8PxyLrv87fz27jP51uRjmntqTQ48aAzz573EE08sBmDEyC2YPXtp8/rnGpcxYuTmvfoeVV1v2W0Tfn/XKyxauJrly9cw47aXaZy9mjfvOoDpP1kOwM0/eonG2bW7GD3x+Eoi4D3HzeVt+zXyza+90JfNV6GPbpO1QepSqLX3lQHq3FmT9uSCT93BW950KRd8+udcelntGxqOOHIsD/xpHuPG/jcH7jOVr37jKLbYYkCrffv378f4t27Hd6fcx0H7fY+XXlrJeZ/av9U2J5y4Kzdc/8jr9n5UX3Z58wA+9omhvPu4uZzwjrnsvscAGhrgW98ZzhXfWcqh+zfy4rI1bDKg9utv1Sq4+3crmHLlNvzsFyP46S3L+fUvXu7jd6F+0f2l3nQaap18ZYA6cer7dueWH/8FgJtufJS99q4NBZ52+h785OZa/cknF/PM0y+w8y6t79XZ2LiMxsalzLznOQBuvulRxo/ftnl9Q0Pwzom7cOMNf26uzXluGaNHD2l+PnLUFsx57sXeeXOqC6eduQW/+v0opt8xkqFD+7HTuE3YeZcB/OinI/jV70fxnpM2Z+wba6ffR45q4ICDNmXr4Q0MHtyPI48exJ/uf+2QuNRXutJTa/MrA3q3WfVj7pwXOeiQ7QF422E78MSs2jc4PPvsUt522I4AbPOGwYwbtzVPP1Ubyrn3T7X7ic6f9xKNs5cxbtxWxf478uifFzQf+7DDx/LYYwt5rnFZc+2ntz7Oe977FgYMaGCHHbdkpzcNaw5FqS3Pz18NwLN/XcWtNy/nvSdt1lxbsyb56pdf4MwP1P5QOuLIQTzy8KssX76GVauSu37zCru8ZUC7x9brw+HHFl2Z/djWVwbs2zvNqbapV03k4IO3Z+vhg3h01jl86Yu/4aNn/4yvfPXv6d+/H6+8spp/Oud/AfjKl+/issuP4+6ZZxERfO6zv2ThwpfZeutBrS6k/OTHb+e7V76TAQMaePqpF/jIpJ82rzvhxLfww+taDz0++ucF/OjGR7nn/g+yetUaPvGx2535qA6dfvI8Fi9aQ/9Ngv/8xtZsObSBy761hO9eVjs3e9zxm/EPZ9TOyw4d1sDZ/7QlRxz4HAQcOWEwRx8zuC+bv9EL6nMYsbuis/tDRsQJwITM/EDx/DRg38w8d63tJgGTAIKhe2028NO90+I6N+GYN7Hj2KFc9u2Zfd2UuvDsksv7ugkSAIcd0Mh9965Y7/EzMnbKDzZ8qdv7f371yfeu6136N2Rd6al19FUCzTJzCjAFoKHfaLsG3fS/P5vV102QVDH21Fp05Zxam18Z0LvNkiRp3XXaU8vMVR18ZYAkqQ8FSdTbXYl7oEu3yWrvKwMkSX2sTq836y7v/ShJFWemtTDUJKnCalP6HX5s4r0fJUl1w56aJFVcj775us4YapJUcYZaC0NNkirOc2otPKcmSaob9tQkqcIiHH4sM9QkqeK8+LqFoSZJFReeU2tmqElSxTn82MKJIpKkumFPTZIqzNtktWaoSVKVOfuxFUNNkiot7amVGGqSVGGBsx/LnCgiSaob9tQkqeLC7kkzQ02Sqiyc/VhmqElSxXlOrYWdVklSuyJiakTMj4iHSrX/jIhHI+KBiLgpIoYW9R0j4uWIuL9YLivts1dEPBgRsyLikojahQgRsVVEzIiIx4t/hxX1KLabVbzOnl1pr6EmSRVWm/3Y/aULrgQmrFWbAeyemXsAjwGfKa17IjPHF8uHS/XJwAeBccXSdMwLgDsycxxwR/Ec4JjStpOK/TtlqElSpdWuU+vu0unRM+8EFq1Vuz0zVxVP7wZGd3SMiBgBDMnMuzMzgauA44vVE4FpxeNpa9Wvypq7gaHFcTpkqElSxUVkt5f14B+Bn5Wej42I+yLi1xFxcFEbBcwubTO7qAFsm5lzisdzgW1L+zzbzj7tcqKIJFVZz2+TNTwiZpaeT8nMKV166Yh/BVYB/1OU5gDbZ+bCiNgL+HFE7NbVhmRmRg+T1lCTpI3bgszce113ioj3A8cBRxRDimTmCmBF8fjeiHgC2BlopPUQ5eiiBjAvIkZk5pxieHF+UW8ExrSzT7scfpSkCmu6S39vnVNr8zUjJgCfBt6ZmctL9W0ioqF4/EZqkzyeLIYXl0bEfsWsx9OBm4vdbgHOKB6fsVb99GIW5H7AktIwZbvsqUlSxfXmdWoRcQ1wKLVhytnAhdRmOw4EZhQz8+8uZjoeAnw+IlYCa4APZ2bTJJOzqc2kHETtHFzTebh/B66PiLOAZ4ATi/p04FhgFrAcOLMr7TXUJKniol/vhVpmntJG+Yp2tr0RuLGddTOB3duoLwSOaKOewDnr1FgMNUmqtHW43myj4Dk1SVLdsKcmSZXml4SWGWqSVHHe0LiFoSZJFWeotfCcmiSpbthTk6QKq92l355aE0NNkqrMb75uxVCTpEpbb3fbrwuGmiRVXDg7opkfhSSpbthTk6QKc6JIa4aaJFVZGGplhpokVZyzH1t4Tk2SVDfsqUlSxTn82MJQk6QKC69Ta8VQk6SKM9RaGGqSVGUB0c9Qa+JEEUlS3bCnJkkV5/BjC0NNkirOUGthqElShTn7sTVDTZKqzNtkteJEEUlS3bCnJkkV570fWxhqklRxEX3dgg2HoSZJFeb3qbXmOTVJUt2wpyZJVeZtslox1CSp0rxOrcxQk6SKM9RaGGqSVHGGWgsnikiS6oahJkkV1jSlv7tLp8ePmBoR8yPioVJtq4iYERGPF/8OK+oREZdExKyIeCAi9iztc0ax/eMRcUapvldEPFjsc0lE7aq79l6jM4aaJFVZ9G6oAVcCE9aqXQDckZnjgDuK5wDHAOOKZRIwGWoBBVwI7AvsA1xYCqnJwAdL+03o5DU6ZKhJUqV1P9C6EmqZeSewaK3yRGBa8XgacHypflXW3A0MjYgRwNHAjMxclJmLgRnAhGLdkMy8OzMTuGqtY7X1Gh1yoogkVVwfTBTZNjPnFI/nAtsWj0cBz5a2m13UOqrPbqPe0Wt0yFCTpI3b8IiYWXo+JTOndHXnzMzo5VRdl9cw1CSp4np4R5EFmbn3Ou4zLyJGZOacYghxflFvBMaUthtd1BqBQ9eq/6qoj25j+45eo0OeU5OkCovenyjSlluAphmMZwA3l+qnF7Mg9wOWFEOItwFHRcSwYoLIUcBtxbqlEbFfMevx9LWO1dZrdMiemiRVXG+O/kXENdR6WcMjYja1WYz/DlwfEWcBzwAnFptPB44FZgHLgTMBMnNRRHwBuKfY7vOZ2TT55GxqMywHAT8rFjp4jQ4ZapKkdmXmKe2sOqKNbRM4p53jTAWmtlGfCezeRn1hW6/RGUNNkirNGxqXGWqSVHGGWgtDTZIqrnZjKUEvhdr43Rdw10+/1xuHltbJK33dAKmXNc1+VI1T+iVJdcPhR0mqOHtqLQw1Saq6nt1RpK4YapJUaU7pLzPUJKnKnCjSihNFJEl1w56aJFWcPbUWhpokVVhgqJUZapJUcYZaC8+pSZLqhj01Sao0p/SXGWqSVGUB4ZhbM0NNkirOnloLQ02SKszZj63ZaZUk1Q17apJUZQHYU2tmqElSxTn82MJQk6RKc0p/maEmSRVnqLVwoogkqW7YU5OkirOn1sJQk6QKC+8o0oqhJklVZ0+tmfkuSaob9tQkqeI8p9bCUJOkSvM6tTJDTZKqLOyplRlqklRxhloLJ4pIkuqGPTVJqrp+9tSaGGqSVGF+SWhrDj9KUpVFcVeRbi6dHj5il4i4v7QsjYh/joiLIqKxVD+2tM9nImJWRPwlIo4u1ScUtVkRcUGpPjYi/lDUr4uIAd39OAw1Saq02pT+7i6dHj3zL5k5PjPHA3sBy4GbitX/1bQuM6cDRMSuwMnAbsAE4NsR0RARDcClwDHArsApxbYAXymO9SZgMXBWdz8NQ02S1FVHAE9k5jMdbDMRuDYzV2TmU8AsYJ9imZWZT2bmq8C1wMSICOBw4IZi/2nA8d1toKEmSRXXmz21tZwMXFN6fm5EPBARUyNiWFEbBTxb2mZ2UWuvvjXwQmauWqveLYaaJFVdZPcXGB4RM0vLpDZfonae653AD4vSZGAnYDwwB/ja6/BOO+XsR0mqsoDo2ZT+BZm5dxe2Owb4Y2bOA2j6FyAiLgduLZ42AmNK+40uarRTXwgMjYj+RW+tvP06s6cmSeqKUygNPUbEiNK6dwEPFY9vAU6OiIERMRYYB/wfcA8wrpjpOIDaUOYtmZnAL4ETiv3PAG7ubiPtqUlShb0e16lFxGbAkcCHSuX/iIjxQAJPN63LzIcj4nrgEWAVcE5mri6Ocy5wG9AATM3Mh4tjnQ9cGxFfBO4DruhuWw01Saq03r9Lf2a+RG1CR7l2WgfbXwxc3EZ9OjC9jfqT1GZH9pihJklV14WLqDcWhpokVZy3yWrhRBFJUt2wpyZJVdbzKf11xVCTpKpz+LGZoSZJFec5tRaeU5Mk1Q17apJUYfE6XKdWJYaaJFVZ4Dm1EkNNkiouPJHUzFCTpIpz+LGF+S5Jqhv21CSp6uypNTPUJKnKwuHHMkNNkiotvU1WiaEmSVVnT62ZE0UkSXXDnpokVVjgObUyQ02Sqiwg/ObrZoaaJFWdE0WaeU5NklQ37KlJUqV5l/4yQ02Sqs5Qa2aoSVKVeUeRVgw1Sao6J4o0c6KIJKlu2FOTpArz4uvWDDVJqjgvvm5hqElSlUV6Tq3EUJOkinP4sYUTRSRJdcOemiRVnT21ZoaaJFVZ4DdflxhqklR19tSaeU6tl3zok/PY/q1PstffP9Ncu/HWZex5xDMM3uFx7v3TK+3ue/UPl7L7IU+z+yFPc/UPl75m/Qn/+Fyr4y56YTVvP3U2ux/yNG8/dTaLX1i9ft+M6sa5k55n3Jhn2H/P2W2uv/SbS9hv/GwO3Hs2EyfM4a/PrGxed8I75rLDts9w0rvmttpnyuSl7Lnrswzb9CkWLvBnrx5FxNMR8WBE3B8RM4vaVhExIyIeL/4dVtQjIi6JiFkR8UBE7Fk6zhnF9o9HxBml+l7F8WcV+3b7IgVDrZec9t4h3HzVyFa13XYZyLVTRnDQvoPa3W/RC6u5+BsLufOWMfzmljFc/I2FrULqxz97kc02a/3f+6uXLubQAwfz0J07cuiBg/nqtxev3zejunHKaZtzwy3btbt+j78dwC9+N5K7Zo7mne/ejIv+teVn6aMf35LLpm7zmn32238gP56+HWO2d+Cnb9Tu0t/dZR0clpnjM3Pv4vkFwB2ZOQ64o3gOcAwwrlgmAZOhFoLAhcC+wD7AhU1BWGzzwdJ+E7r7aRhqveSgfQex1dCGVrU3jxvAzjsN6HC/Gb9ezhEHD2aroQ0MG9rAEQcP5vZfLwfgxZfWcMnli7ngo1u12ufWGS/yvhOGAPC+E4bwk9tfXI/vRPXkwIMHMWxY+/+3P/jQQQweXFv/d/sMpHH2quZ1bzt8EFts/to/oPcYP5Dtd9xk/TdWXdcvu79030RgWvF4GnB8qX5V1twNDI2IEcDRwIzMXJSZi4EZwIRi3ZDMvDszE7iqdKx1ZqhtYJ6bu4rRI1t+QYwa0Z/n5tZ+sfzbVxfysUnDGDyo9X+2+QtWM2Lb2l/J272hgfkOAWk9+P6Vyzjy6PZHFbRhiOjZ0kUJ3B4R90bEpKK2bWbOKR7PBbYtHo8Cni3tO7uodVSf3Ua9WzoNtYiYGhHzI+Kh7r6Ieu5PD6/gqWdWMnHC5h1uFxF4xxz11HU/eJH7//gqHz1vaF83RV0R2f0FhkfEzNIyqY1XOCgz96Q2tHhORBxSXln0sDaI2Spd6aldSQ/GN7VuRm7Xn9nPtZycb5yzipHb9ecPf3yZex94hV0OeIrD3zObx596laNOrP1x84bhDcyZV+vNzZm3im2GN7R5bKkrfnXHy3z9Ky/wgxu2ZeBA/0TaCCzIzL1Ly5S1N8jMxuLf+cBN1M6JzSuGDin+nV9s3giMKe0+uqh1VB/dRr1bOg21zLwTWNTdF1DnGueu4piTawF15NsG8/PfLGfxC6tZ/MJqfv6b5Rz5tsFMOm0oT818I3/53Vh+ceNoxo0dwO3X134O3n7kZlx9Q22W5NU3LOW4IzvuzUllUyYvZcrk2s/PA/ev4OPnLuAHN27LNm/wj6PK6MVzahGxWURs0fQYOAp4CLgFaJrBeAZwc/H4FuD0YhbkfsCSYgv0hEwAAAgSSURBVJjyNuCoiBhWTBA5CritWLc0IvYrZj2eXjrWOnO6Ui85/dw5/Ob3L7Ng8Wp22ucp/t95WzFsaAPnfe55FixazbvPfI49dh3IT64exdx5q+jfv/YX8VZDG/jMP23FQe+oDT3/y8e2es2Ek7V98uyteN9H5jDtuqVsP6o/V08e0evvT9V01mnzues3r7BwwWp22+mvXPDZYTz+l1fZd/9NAfjcZxbx0ktreP+ptT+6R4/pzzU31k6VHHP4czz+2EpeejHZbae/csllwzniyMF859IlXPL1Jcybu5qD/q6RI48exCWXvXaWpHpPL9/7cVvgpmKWfX/gB5n5vxFxD3B9RJwFPAOcWGw/HTgWmAUsB84EyMxFEfEF4J5iu89nZlOH6Wxqo4KDgJ8VS7dEbSi0k40idgRuzczdO9hmErXpm4wZ1X+vx34/trtt2uhMvvIFxozsz3FH2cNa3155w6rON9rInfSuuXz/um0ZMMChxt502AGN3HfvivX+Ie+1wxb5u/P37nzDdmx6zq/uLU3Tr7z11lMrxmGnAOy1x6YbxAnDqvjI+z0Zr75z3U3tX7emavA2WS2c0i9JqhtdmdJ/DfB7YJeImF2Mn0qSNhQ9m9JfVzodfszMU16PhkiSuiFwzK3E2Y+SVHF+83UL812SVDfsqUlS1dlTa2aoSVKl9fhu+3XFUJOkKgvPqZUZapJUdfbUmjlRRJJUN+ypSVLVOfzYzFCTpApbx2+wrnuGmiRVnefUmhlqklRp9XkPx+5yoogkqW7YU5OkKgscfiwx1CSp6hx+bGaoSVLF+c3XLTynJkmqG/bUJKnqvE6tmaEmSVXmRJFWDDVJqjSvUysz1CSp6uypNXOiiCSpbthTk6QqCxx+LDHUJKnqHHNrZqhJUtXZU2tmqElSlYV3FCmz0ypJqhv21CSp0rxOrcxQk6Sqc8ytmaEmSVVnT62Z+S5Jqhv21CSpyryhcSv21CSp6qIHS2eHjhgTEb+MiEci4uGI+FhRvygiGiPi/mI5trTPZyJiVkT8JSKOLtUnFLVZEXFBqT42Iv5Q1K+LiAHd/SgMNUmqun7Z/aVzq4BPZOauwH7AORGxa7HuvzJzfLFMByjWnQzsBkwAvh0RDRHRAFwKHAPsCpxSOs5XimO9CVgMnNXtj6K7O0qSNgAB2a/7S2cyc05m/rF4vAz4MzCqg10mAtdm5orMfAqYBexTLLMy88nMfBW4FpgYEQEcDtxQ7D8NOL57H4ahJknqoojYEXgr8IeidG5EPBARUyNiWFEbBTxb2m12UWuvvjXwQmauWqveLYaaJFVdZPcXGB4RM0vLpDZfImJz4EbgnzNzKTAZ2AkYD8wBvvY6vdsOOftRkiqty+fG2rMgM/fuaIOI2IRaoP1PZv4IIDPnldZfDtxaPG0ExpR2H13UaKe+EBgaEf2L3lp5+3VmT02SqqwnMx+7NvsxgCuAP2fm10v1EaXN3gU8VDy+BTg5IgZGxFhgHPB/wD3AuGKm4wBqk0luycwEfgmcUOx/BnDzOn0GJfbUJKnqerd7ciBwGvBgRNxf1P6F2uzF8UACTwMfAsjMhyPieuARajMnz8nM1QARcS5wG9AATM3Mh4vjnQ9cGxFfBO6jFqLdYqhJktqVmb+l7T7d9A72uRi4uI369Lb2y8wnqc2O7DFDTZKqzjuKNDPUJKnKunhubGNhqElSxaU9tWbOfpQk1Q17apJUdQ4/NjPUJKnKAsfcSgw1Sao6Q62ZoSZJFZZAhhNFmpjvkqS6YU9NkqrMc2qtGGqSVHWGWjNDTZIqLp3S38x8lyTVDXtqklRlnlNrxVCTpIpLQ62ZoSZJVReeVGtiqElSlYU9tTI/CklS3bCnJklVZ/ekmaEmSRXndWotDDVJqjKn9LdiqElShSWQ/eyqNTHfJUl1w56aJFWd3ZNmhpokVVk4UaTMUJOkqvOcWjM7rZKkumFPTZIqzttktTDUJKnKPKfWiqEmSVVnT62ZoSZJVeZd+lvxo5Ak1Q17apJUYbXbZGVfN2ODYahJUtWFodbEUJOkKgt7amWGmiRVWkK/1X3diA1GZK7/hI+I54Fn1vuBNy7DgQV93QgJfxbXlx0yc5v1fdCI+F9q/426a0FmTlhf7elrvRJq6rmImJmZe/d1OyR/FlUlTumXJNUNQ02SVDcMtQ3XlL5ugFTwZ1GV4Tk1SVLdsKcmSaobhpokqW4YahuQiNglIvaPiE0ioqGv2yNJVeM5tQ1ERLwb+BLQWCwzgSszc2mfNkwbrYhoyExvVaFKsae2AYiITYCTgLMy8wjgZmAMcH5EDOnTxmmjExE7A2TmakcMVDWG2oZjCDCueHwTcCuwCXBqRPhl7XpdRMRxwP0R8QMw2FQ9htoGIDNXAl8H3h0RB2fmGuC3wP3AQX3aOG00ImIz4Fzgn4FXI+JqMNhULZ5T20BExKbAB4A9gKsz886i/gvgvMy8vy/bp41DRIwElgKbApcBr2Tm+/q2VVLXGWobkIgYBpwKHEdtCHIF8Gng8Myc15dt08YnIramdjeRlzPzfRGxJ7A8Mx/t46ZJ7TLUNjARMQA4EPgQ8Arwzcy8r29bpY1VRAwH/hPYH2gADsvM2X3bKql9htoGqjiHkcX5NanPRMTHgfOBIzPzwb5uj9QRv/l6A+X1QdoQFEPixwJHGWiqAntqkjoUEZtm5it93Q6pKww1SVLd8Do1SVLdMNQkSXXDUJMk1Q1DTZJUNww1SVLdMNQkSXXj/wPuXZdNBRAQjQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p3MFU8gvtAN"
      },
      "source": [
        "with open('Binary_Classification.pkl', 'rb') as f:\n",
        "  clf = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1F_CyHUxEFS"
      },
      "source": [
        "# Prob predictions for actual test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1XWTy4BxHG6"
      },
      "source": [
        "train_prob = clf.predict(train)\n",
        "test_prob = clf.predict(test)\n",
        "\n",
        "threshold = 0.030\n",
        "\n",
        "train_labels = np.where(train_prob > threshold, 1, 0)\n",
        "test_labels = np.where(test_prob > threshold, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yThsNDLVxX-o",
        "outputId": "c8a4d131-6248-4fef-d8b9-74601dd7f59f"
      },
      "source": [
        "print(\"Accuracy on entire train data:\", accuracy_score(train_labels, y_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on entire train data: 0.9403913489205961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmFzUZVYxo_w"
      },
      "source": [
        "# Saving our predictions for further use\n",
        "with open('prob_labels_train.pkl', 'wb') as f:\n",
        "  pickle.dump(train_prob, f)\n",
        "\n",
        "with open('prob_labels_test.pkl', 'wb') as f:\n",
        "  pickle.dump(test_prob, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABZCdoJUycix"
      },
      "source": [
        "# Saving our predictions for further use\n",
        "with open('predicted_labels_train.pkl', 'wb') as f:\n",
        "  pickle.dump(train_labels, f)\n",
        "\n",
        "with open('predicted_labels_test.pkl', 'wb') as f:\n",
        "  pickle.dump(test_labels, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXkkhcYyeA-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}